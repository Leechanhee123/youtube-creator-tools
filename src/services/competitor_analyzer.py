from typing import Dict, List, Any, Optional
import logging
from datetime import datetime, timedelta
from src.services.youtube_data_api import YouTubeDataAPIService

logger = logging.getLogger(__name__)

class CompetitorAnalyzer:
    """ê²½ìŸì‚¬ ë¶„ì„ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.youtube_service = YouTubeDataAPIService()
    
    async def analyze_competitors(self, 
                                target_channel_id: str,
                                competitor_urls: List[str],
                                analysis_period: str = "30d") -> Dict[str, Any]:
        """
        ê²½ìŸì‚¬ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            target_channel_id: ë¶„ì„ ëŒ€ìƒ ì±„ë„ ID
            competitor_urls: ê²½ìŸì‚¬ ì±„ë„ URL ëª©ë¡
            analysis_period: ë¶„ì„ ê¸°ê°„ (7d, 30d, 90d)
            
        Returns:
            ê²½ìŸì‚¬ ë¶„ì„ ê²°ê³¼
        """
        try:
            logger.info(f"Starting competitor analysis for channel: {target_channel_id}")
            
            # 1ë‹¨ê³„: ëŒ€ìƒ ì±„ë„ ì •ë³´ ì¡°íšŒ
            target_channel_info = await self.youtube_service.get_channel_info(channel_id=target_channel_id)
            
            if not target_channel_info.get('success'):
                return {
                    'success': False,
                    'message': 'ëŒ€ìƒ ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                    'data': None
                }
            
            target_data = target_channel_info['data']
            
            # 2ë‹¨ê³„: ê²½ìŸì‚¬ URLì—ì„œ ì±„ë„ ì •ë³´ ìˆ˜ì§‘
            competitors = await self._get_competitors_from_urls(competitor_urls)
            
            # 4ë‹¨ê³„: ê° ê²½ìŸì‚¬ì˜ ìƒì„¸ ë¶„ì„
            competitor_analyses = []
            for competitor in competitors:
                analysis = await self._analyze_single_competitor(
                    target_data=target_data,
                    competitor_data=competitor,
                    analysis_period=analysis_period
                )
                if analysis:
                    competitor_analyses.append(analysis)
            
            # 5ë‹¨ê³„: ì „ëµì  ì œì•ˆ ìƒì„±
            strategic_recommendations = self._generate_strategic_recommendations(
                target_data=target_data,
                competitor_analyses=competitor_analyses
            )
            
            # 6ë‹¨ê³„: ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
            market_insights = self._generate_market_insights(
                target_data=target_data,
                competitor_analyses=competitor_analyses
            )
            
            logger.info(f"Competitor analysis completed for {target_channel_id}. Found {len(competitor_analyses)} competitors")
            
            return {
                'success': True,
                'message': f'{len(competitor_analyses)}ê°œì˜ ê²½ìŸ ì±„ë„ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
                'data': {
                    'target_channel': {
                        'channel_id': target_data['channel_id'],
                        'title': target_data['title'],
                        'subscriber_count': target_data['statistics']['subscriber_count'],
                        'video_count': target_data['statistics']['video_count'],
                        'view_count': target_data['statistics']['view_count'],
                        'topic_categories': target_data.get('topic_details', {}).get('topic_categories', [])
                    },
                    'competitors': competitor_analyses,
                    'strategic_recommendations': strategic_recommendations,
                    'market_insights': market_insights,
                    'analysis_metadata': {
                        'analysis_period': analysis_period,
                        'analyzed_at': datetime.now().isoformat(),
                        'total_competitors_found': len(competitor_analyses)
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"Competitor analysis failed for {target_channel_id}: {str(e)}")
            return {
                'success': False,
                'message': f'ê²½ìŸì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}',
                'data': None
            }
    
    async def _get_competitors_from_urls(self, competitor_urls: List[str]) -> List[Dict[str, Any]]:
        """ê²½ìŸì‚¬ URL ëª©ë¡ì—ì„œ ì±„ë„ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        competitors = []
        
        for url in competitor_urls:
            try:
                # URLì—ì„œ ì±„ë„ ì •ë³´ ì¶”ì¶œ
                channel_info = await self.youtube_service.get_channel_info(url=url)
                
                if channel_info.get('success') and channel_info.get('data'):
                    competitors.append(channel_info['data'])
                else:
                    logger.warning(f"Failed to get channel info for URL: {url}")
                    
            except Exception as e:
                logger.error(f"Error processing competitor URL {url}: {str(e)}")
                continue
        
        return competitors
    
    async def _extract_topic_keywords(self, channel_data: Dict[str, Any]) -> List[str]:
        """ì±„ë„ ë°ì´í„°ì—ì„œ ì£¼ì œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        keywords = []
        
        # ì±„ë„ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        title = channel_data.get('title', '')
        title_keywords = self._extract_keywords_from_text(title)
        keywords.extend(title_keywords)
        
        # ì±„ë„ ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        description = channel_data.get('description', '')
        desc_keywords = self._extract_keywords_from_text(description)
        keywords.extend(desc_keywords[:5])  # ìƒìœ„ 5ê°œë§Œ
        
        # ë¸Œëœë”© í‚¤ì›Œë“œ ì¶”ì¶œ
        branding_keywords = channel_data.get('branding', {}).get('keywords', '')
        if branding_keywords:
            brand_keywords = [kw.strip() for kw in branding_keywords.split(',')]
            keywords.extend(brand_keywords[:3])  # ìƒìœ„ 3ê°œë§Œ
        
        # topicCategoriesì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        topic_categories = channel_data.get('topic_details', {}).get('topic_categories', [])
        for topic_url in topic_categories:
            topic_keyword = self._extract_keyword_from_wikipedia_url(topic_url)
            if topic_keyword:
                keywords.append(topic_keyword)
        
        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ í‚¤ì›Œë“œë§Œ ë°˜í™˜
        unique_keywords = list(dict.fromkeys(keywords))  # ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°
        return unique_keywords[:8]  # ìµœëŒ€ 8ê°œ í‚¤ì›Œë“œ
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not text:
            return []
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ì‚¬ìš© ê°€ëŠ¥)
        common_words = {'ì±„ë„', 'êµ¬ë…', 'ì¢‹ì•„ìš”', 'ëŒ“ê¸€', 'ì˜ìƒ', 'ë¹„ë””ì˜¤', 'channel', 'subscribe', 'like', 'comment', 'video'}
        
        words = text.lower().split()
        keywords = []
        
        for word in words:
            # íŠ¹ìˆ˜ë¬¸ì ì œê±°
            clean_word = ''.join(c for c in word if c.isalnum() or c in 'ê°€-í£')
            # ê¸¸ì´ ì²´í¬ ë° ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
            if len(clean_word) >= 2 and clean_word not in common_words:
                keywords.append(clean_word)
        
        return keywords[:5]  # ìƒìœ„ 5ê°œë§Œ
    
    def _extract_keyword_from_wikipedia_url(self, url: str) -> Optional[str]:
        """Wikipedia URLì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # URLì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ì¶œ (ì˜ˆ: "Video_game" -> "ê²Œì„")
            keyword = url.split('/')[-1].replace('_', ' ')
            
            # ì˜ì–´ í‚¤ì›Œë“œë¥¼ í•œêµ­ì–´ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ë§¤í•‘)
            keyword_mapping = {
                'Video game': 'ê²Œì„',
                'Music': 'ìŒì•…',
                'Entertainment': 'ì—”í„°í…Œì¸ë¨¼íŠ¸',
                'Education': 'êµìœ¡',
                'Technology': 'ê¸°ìˆ ',
                'Sports': 'ìŠ¤í¬ì¸ ',
                'Comedy': 'ì½”ë¯¸ë””',
                'Gaming': 'ê²Œì„',
                'Film': 'ì˜í™”',
                'Television': 'TV'
            }
            
            return keyword_mapping.get(keyword, keyword.lower())
        except:
            return None
    
    async def _find_similar_channels(self, 
                                   target_data: Dict[str, Any],
                                   topic_keywords: List[str],
                                   max_results: int) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ì±„ë„ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            # í‚¤ì›Œë“œë¡œ ì±„ë„ ê²€ìƒ‰
            search_result = await self.youtube_service.search_channels_by_topic(
                topic_keywords=topic_keywords,
                max_results=max_results * 2,  # ë” ë§ì´ ê²€ìƒ‰í•´ì„œ í•„í„°ë§
                region='KR'
            )
            
            if not search_result.get('success'):
                return []
            
            channels = search_result['data']['channels']
            target_channel_id = target_data['channel_id']
            
            # ëŒ€ìƒ ì±„ë„ ì œì™¸
            filtered_channels = [ch for ch in channels if ch['channel_id'] != target_channel_id]
            
            # ê° ì±„ë„ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ
            detailed_channels = []
            for channel in filtered_channels[:max_results]:
                channel_info = await self.youtube_service.get_channel_info(channel_id=channel['channel_id'])
                if channel_info.get('success'):
                    detailed_channels.append(channel_info['data'])
            
            return detailed_channels
            
        except Exception as e:
            logger.error(f"Error finding similar channels: {str(e)}")
            return []
    
    async def _analyze_single_competitor(self, 
                                       target_data: Dict[str, Any],
                                       competitor_data: Dict[str, Any],
                                       analysis_period: str) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ê²½ìŸì‚¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ë³¸ ì„±ê³¼ ë¹„êµ
            performance_comparison = self._calculate_performance_comparison(target_data, competitor_data)
            
            # ì±„ë„ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_score = self._calculate_channel_similarity(target_data, competitor_data)
            
            # ê²½ìŸì‚¬ì˜ ìµœê·¼ ë¹„ë””ì˜¤ ë¶„ì„
            recent_videos = await self.youtube_service.get_channel_videos(
                channel_id=competitor_data['channel_id'],
                max_results=20,
                order='date'
            )
            
            content_insights = {}
            if recent_videos.get('success'):
                content_insights = self._analyze_content_strategy(recent_videos['data']['videos'])
            
            return {
                'channel_id': competitor_data['channel_id'],
                'title': competitor_data['title'],
                'similarity_score': similarity_score,
                'performance_comparison': performance_comparison,
                'content_insights': content_insights,
                'channel_stats': {
                    'subscriber_count': competitor_data['statistics']['subscriber_count'],
                    'view_count': competitor_data['statistics']['view_count'],
                    'video_count': competitor_data['statistics']['video_count']
                }
            }
            
        except Exception as e:
            logger.error(f"Error analyzing competitor {competitor_data.get('channel_id', 'unknown')}: {str(e)}")
            return None
    
    def _calculate_performance_comparison(self, target_data: Dict[str, Any], competitor_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ê³¼ ë¹„êµë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        target_stats = target_data['statistics']
        competitor_stats = competitor_data['statistics']
        
        # ì•ˆì „í•œ ë‚˜ëˆ—ì…ˆì„ ìœ„í•œ í•¨ìˆ˜
        def safe_ratio(a, b):
            return a / b if b > 0 else 0
        
        return {
            'subscriber_ratio': safe_ratio(competitor_stats['subscriber_count'], target_stats['subscriber_count']),
            'view_ratio': safe_ratio(competitor_stats['view_count'], target_stats['view_count']),
            'video_ratio': safe_ratio(competitor_stats['video_count'], target_stats['video_count']),
            'avg_views_per_video_ratio': safe_ratio(
                safe_ratio(competitor_stats['view_count'], competitor_stats['video_count']),
                safe_ratio(target_stats['view_count'], target_stats['video_count'])
            )
        }
    
    def _calculate_channel_similarity(self, target_data: Dict[str, Any], competitor_data: Dict[str, Any]) -> float:
        """ì±„ë„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        similarity_score = 0.0
        
        # ì£¼ì œ ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ (50% ê°€ì¤‘ì¹˜)
        target_topics = set(target_data.get('topic_details', {}).get('topic_categories', []))
        competitor_topics = set(competitor_data.get('topic_details', {}).get('topic_categories', []))
        
        if target_topics and competitor_topics:
            topic_overlap = len(target_topics.intersection(competitor_topics))
            topic_union = len(target_topics.union(competitor_topics))
            topic_similarity = topic_overlap / topic_union if topic_union > 0 else 0
            similarity_score += topic_similarity * 0.5
        
        # í‚¤ì›Œë“œ ìœ ì‚¬ë„ (30% ê°€ì¤‘ì¹˜)
        target_keywords = set(self._extract_keywords_from_text(
            f"{target_data.get('title', '')} {target_data.get('description', '')}"
        ))
        competitor_keywords = set(self._extract_keywords_from_text(
            f"{competitor_data.get('title', '')} {competitor_data.get('description', '')}"
        ))
        
        if target_keywords and competitor_keywords:
            keyword_overlap = len(target_keywords.intersection(competitor_keywords))
            keyword_union = len(target_keywords.union(competitor_keywords))
            keyword_similarity = keyword_overlap / keyword_union if keyword_union > 0 else 0
            similarity_score += keyword_similarity * 0.3
        
        # ê·œëª¨ ìœ ì‚¬ë„ (20% ê°€ì¤‘ì¹˜)
        target_subs = target_data['statistics']['subscriber_count']
        competitor_subs = competitor_data['statistics']['subscriber_count']
        
        if target_subs > 0 and competitor_subs > 0:
            size_ratio = min(target_subs, competitor_subs) / max(target_subs, competitor_subs)
            similarity_score += size_ratio * 0.2
        
        return min(similarity_score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    def _analyze_content_strategy(self, videos: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì½˜í…ì¸  ì „ëµì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        if not videos:
            return {}
        
        # ì œëª© íŒ¨í„´ ë¶„ì„
        title_patterns = []
        total_length = 0
        
        for video in videos:
            title = video.get('title', '')
            total_length += len(title)
            
            # íŠ¹ìˆ˜ë¬¸ì ì‚¬ìš© íŒ¨í„´
            if any(char in title for char in ['âœ¨', 'ğŸ”¥', 'â¤ï¸', 'ğŸ’', 'â­']):
                title_patterns.append('ì´ëª¨ì§€ ì‚¬ìš©')
            if '!' in title or '?' in title:
                title_patterns.append('ê°ì • í‘œí˜„')
            if any(word in title.upper() for word in ['NEW', 'ì‹ ì‘', 'ìµœì‹ ', 'ì—…ë°ì´íŠ¸']):
                title_patterns.append('ì‹ ê·œì„± ê°•ì¡°')
        
        avg_title_length = total_length / len(videos) if videos else 0
        
        # ì—…ë¡œë“œ íŒ¨í„´ ë¶„ì„
        upload_pattern = self._analyze_upload_pattern(videos)
        
        return {
            'avg_title_length': round(avg_title_length, 1),
            'common_title_patterns': list(set(title_patterns)),
            'upload_pattern': upload_pattern,
            'recent_video_count': len(videos)
        }
    
    def _analyze_upload_pattern(self, videos: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì—…ë¡œë“œ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        if not videos:
            return {}
        
        upload_days = []
        upload_hours = []
        
        for video in videos:
            published_at = video.get('published_at')
            if published_at:
                try:
                    dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                    upload_days.append(dt.strftime('%A'))
                    upload_hours.append(dt.hour)
                except:
                    continue
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ì—…ë¡œë“œ ìš”ì¼ê³¼ ì‹œê°„
        most_common_day = max(set(upload_days), key=upload_days.count) if upload_days else None
        avg_hour = sum(upload_hours) / len(upload_hours) if upload_hours else None
        
        return {
            'most_common_upload_day': most_common_day,
            'avg_upload_hour': round(avg_hour, 1) if avg_hour else None,
            'upload_frequency': f"ìµœê·¼ {len(videos)}ê°œ ì˜ìƒ"
        }
    
    def _generate_strategic_recommendations(self, 
                                          target_data: Dict[str, Any],
                                          competitor_analyses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì „ëµì  ì œì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤."""
        recommendations = []
        
        if not competitor_analyses:
            return recommendations
        
        # ì„±ê³¼ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
        better_performers = [comp for comp in competitor_analyses 
                           if comp['performance_comparison']['subscriber_ratio'] > 1.2]
        
        if better_performers:
            # êµ¬ë…ì ìˆ˜ê°€ ë” ë§ì€ ê²½ìŸì‚¬ì˜ ê³µí†µ íŒ¨í„´ ë¶„ì„
            common_patterns = []
            
            for comp in better_performers:
                patterns = comp.get('content_insights', {}).get('common_title_patterns', [])
                common_patterns.extend(patterns)
            
            if common_patterns:
                most_common = max(set(common_patterns), key=common_patterns.count)
                recommendations.append({
                    'priority': 'high',
                    'type': 'content_strategy',
                    'suggestion': f'ìƒìœ„ ê²½ìŸì‚¬ë“¤ì´ ìì£¼ ì‚¬ìš©í•˜ëŠ” "{most_common}" íŒ¨í„´ í™œìš© ê¶Œì¥',
                    'impact': 'subscriber_growth'
                })
        
        # ì—…ë¡œë“œ ë¹ˆë„ ë¶„ì„
        target_video_count = target_data['statistics']['video_count']
        avg_competitor_videos = sum(comp['channel_stats']['video_count'] 
                                  for comp in competitor_analyses) / len(competitor_analyses)
        
        if avg_competitor_videos > target_video_count * 1.5:
            recommendations.append({
                'priority': 'medium',
                'type': 'upload_frequency',
                'suggestion': f'ê²½ìŸì‚¬ í‰ê·  ëŒ€ë¹„ ì—…ë¡œë“œ ë¹ˆë„ ì¦ê°€ í•„ìš” (í˜„ì¬: {target_video_count}ê°œ, í‰ê· : {int(avg_competitor_videos)}ê°œ)',
                'impact': 'visibility'
            })
        
        # ì œëª© ê¸¸ì´ ë¶„ì„
        avg_title_lengths = [comp.get('content_insights', {}).get('avg_title_length', 0) 
                           for comp in competitor_analyses if comp.get('content_insights', {}).get('avg_title_length')]
        
        if avg_title_lengths:
            optimal_length = sum(avg_title_lengths) / len(avg_title_lengths)
            recommendations.append({
                'priority': 'low',
                'type': 'title_optimization',
                'suggestion': f'ì œëª© ê¸¸ì´ ìµœì í™” ê¶Œì¥ (ê¶Œì¥ ê¸¸ì´: {int(optimal_length)}ì)',
                'impact': 'click_through_rate'
            })
        
        return recommendations
    
    def _generate_market_insights(self, 
                                target_data: Dict[str, Any],
                                competitor_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì‹œì¥ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not competitor_analyses:
            return {
                'market_position': 'unknown',
                'total_competitors_analyzed': 0,
                'growth_opportunities': ['ê²½ìŸì‚¬ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ë¶„ì„ ë¶ˆê°€'],
                'market_avg_subscribers': 0,
                'competitive_advantage': 'ë¶„ì„ ëŒ€ìƒ ê²½ìŸì‚¬ ì—†ìŒ'
            }
        
        # ì‹œì¥ ìœ„ì¹˜ ë¶„ì„
        target_subs = target_data['statistics']['subscriber_count']
        competitor_subs = [comp['channel_stats']['subscriber_count'] for comp in competitor_analyses]
        
        market_position = 'top'
        better_count = sum(1 for subs in competitor_subs if subs > target_subs)
        
        if better_count > len(competitor_subs) * 0.7:
            market_position = 'bottom'
        elif better_count > len(competitor_subs) * 0.3:
            market_position = 'middle'
        
        # ì„±ì¥ ê¸°íšŒ ë¶„ì„
        growth_opportunities = []
        
        # í‰ê·  ì¡°íšŒìˆ˜ ë¶„ì„
        target_avg_views = target_data['statistics']['view_count'] / max(target_data['statistics']['video_count'], 1)
        competitor_avg_views = []
        
        for comp in competitor_analyses:
            comp_stats = comp['channel_stats']
            comp_avg = comp_stats['view_count'] / max(comp_stats['video_count'], 1)
            competitor_avg_views.append(comp_avg)
        
        if competitor_avg_views:
            market_avg_views = sum(competitor_avg_views) / len(competitor_avg_views)
            if market_avg_views > target_avg_views * 1.5:
                growth_opportunities.append('í‰ê·  ì¡°íšŒìˆ˜ ê°œì„  ì—¬ì§€ í¼')
        
        return {
            'market_position': market_position,
            'total_competitors_analyzed': len(competitor_analyses),
            'growth_opportunities': growth_opportunities,
            'market_avg_subscribers': int(sum(competitor_subs) / len(competitor_subs)) if competitor_subs else 0,
            'competitive_advantage': 'ë¶„ì„ëœ ê²½ìŸì‚¬ ì¤‘ ìƒìœ„ê¶Œ' if market_position == 'top' else 'ì„±ì¥ ê°€ëŠ¥ì„± ë†’ìŒ'
        }